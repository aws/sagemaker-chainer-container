{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training with Chainer and ChainerMN\n",
    "\n",
    "Chainer can train in two modes: single-machine, and distributed. Unlike the single-machine notebook example that trains an image classification model on the CIFAR-10 dataset, we will write a Chainer script that uses `chainermn` to distribute training to multiple instances.\n",
    "\n",
    "[VGG](https://arxiv.org/pdf/1409.1556v6.pdf) is an architecture for deep convolution networks. In this example, we train a convolutional network to perform image classification using the CIFAR-10 dataset on multiple instances. CIFAR-10 consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. We'll train a model on SageMaker, deploy it to Amazon SageMaker, and then classify images using the deployed model.\n",
    "\n",
    "To train with a Chainer script, we construct a ```Chainer``` estimator using the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk). We can pass in an `entry_point`, the name of a script that contains a couple of functions with certain signatures (`train` and `model_fn`). This script will be run on SageMaker in a container that invokes these functions to train and load Chainer models. \n",
    "\n",
    "For more on the Chainer container, please visit the sagemaker-chainer-containers repository:\n",
    "https://github.com/aws/sagemaker-chainer-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This role retrieves the SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading training and test data\n",
    "\n",
    "We use helper functions given by `chainer` to download and preprocess the CIFAR10 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "\n",
    "from chainer.datasets import get_cifar10\n",
    "\n",
    "train, test = get_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We save the preprocessed data to the local filesystem, and then use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the S3 location, which we will use when we start the Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data = [element[0] for element in train]\n",
    "train_labels = [element[1] for element in train]\n",
    "\n",
    "test_data = [element[0] for element in test]\n",
    "test_labels = [element[1] for element in test]\n",
    "\n",
    "try:\n",
    "    os.makedirs('data/train')\n",
    "    os.makedirs('data/test')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "np.savez('data/train/train.npz', data=train_data, labels=train_labels)\n",
    "np.savez('data/test/test.npz', data=test_data, labels=test_labels)\n",
    "\n",
    "# Upload preprocessed data to S3 \n",
    "train_input = sagemaker_session.upload_data(path=os.path.join('data', 'train'),\n",
    "                                                            key_prefix='notebook/chainer_cifar/train')\n",
    "test_input = sagemaker_session.upload_data(path=os.path.join('data', 'test'),\n",
    "                                                           key_prefix='notebook/chainer_cifar/test')\n",
    "\n",
    "# Remove data from notebook instance (to save disk space)\n",
    "shutil.rmtree('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Chainer training script to run on Amazon SageMaker\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a function `train` that returns a trained model.\n",
    "\n",
    "Since we will use the same script to host the Chainer model, the script also needs a function `model_fn` that loads the model -- by default, Chainer models are saved to disk as `model.npz`. When SageMaker calls your `train` and `model_fn` functions, it will pass in arguments that describe the training environment.\n",
    "\n",
    "The script below uses a `chainermn` Communicator to distribute training to multiple nodes. The Communicator depends on MPI (Message Passing Interface), so the Chainer container running on SageMaker runs this script with `mpirun` if the `Chainer` Estimator specifies a `train_instance_count` of two or greater, or if `use_mpi` in the `Chainer` estimator is `true`.\n",
    "\n",
    "By default, one process is created per GPU (on GPU instances), or one per host (on CPU instances, which are not recommended for this notebook).\n",
    "\n",
    "See https://github.com/chainer/chainermn for more on running Chainer on multiple nodes. Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n",
      "#\r\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\r\n",
      "# may not use this file except in compliance with the License. A copy of\r\n",
      "# the License is located at\r\n",
      "#\r\n",
      "#     http://aws.amazon.com/apache2.0/\r\n",
      "#\r\n",
      "# or in the \"license\" file accompanying this file. This file is\r\n",
      "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\r\n",
      "# ANY KIND, either express or implied. See the License for the specific\r\n",
      "# language governing permissions and limitations under the License.\r\n",
      "\r\n",
      "from __future__ import print_function, absolute_import\r\n",
      "\r\n",
      "import os\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import chainer\r\n",
      "import chainer.functions as F\r\n",
      "import chainer.links as L\r\n",
      "import chainermn\r\n",
      "from chainer import initializers\r\n",
      "from chainer import serializers\r\n",
      "from chainer import training\r\n",
      "from chainer.training import extensions\r\n",
      "\r\n",
      "import net\r\n",
      "\r\n",
      "\r\n",
      "def train(hyperparameters, num_gpus, output_data_dir, channel_input_dirs):\r\n",
      "    \"\"\"\r\n",
      "    This function is called by the Chainer container during training when running on SageMaker with\r\n",
      "    values populated by the training environment.\r\n",
      "    \r\n",
      "    When running in distributed mode, this function is called with `mpirun`, spawning (by default) one\r\n",
      "    process per GPU (when running with GPU instances), or one process per host (when running with\r\n",
      "    CPU instances). Ranks are initialized in the expected way -- comm.intra_rank refers to the rank of the\r\n",
      "    process on an instance, and comm.inter_rank refers to the rank of the instance.\r\n",
      "\r\n",
      "    Args:\r\n",
      "        hyperparameters (dict): map of hyperparameters given to the training job.\r\n",
      "        num_gpus (int): number of gpus available to the container, determined by instance type.\r\n",
      "        output_data_dir (str): path to the directory to write output artifacts to\r\n",
      "        channel_input_dirs (dict): Dictionary mapping input channel names to local filesystem paths\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        a trained Chainer model\r\n",
      "    \r\n",
      "    For more on `train`, please visit the sagemaker-python-sdk repository:\r\n",
      "    https://github.com/aws/sagemaker-python-sdk\r\n",
      "    \r\n",
      "    For more on the Chainer container, please visit the sagemaker-chainer-containers repository:\r\n",
      "    https://github.com/aws/sagemaker-chainer-containers\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    train_data = np.load(os.path.join(channel_input_dirs['train'], 'train.npz'))['data']\r\n",
      "    train_labels = np.load(os.path.join(channel_input_dirs['train'], 'train.npz'))['labels']\r\n",
      "\r\n",
      "    test_data = np.load(os.path.join(channel_input_dirs['test'], 'test.npz'))['data']\r\n",
      "    test_labels = np.load(os.path.join(channel_input_dirs['test'], 'test.npz'))['labels']\r\n",
      "\r\n",
      "    train = chainer.datasets.TupleDataset(train_data, train_labels)\r\n",
      "    test = chainer.datasets.TupleDataset(test_data, test_labels)\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 256)\r\n",
      "    epochs = hyperparameters.get('epochs', 50)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.05)\r\n",
      "    communicator = hyperparameters.get('communicator', 'pure_nccl' if num_gpus > 0 else 'naive')\r\n",
      "\r\n",
      "    comm = chainermn.create_communicator(communicator)\r\n",
      "\r\n",
      "    if comm.inter_rank == 0:\r\n",
      "        print('# Minibatch-size: {}'.format(batch_size))\r\n",
      "        print('# epoch: {}'.format(epochs))\r\n",
      "        print('# learning rate: {}'.format(learning_rate))\r\n",
      "        print('# communicator: {}'.format(communicator))\r\n",
      "\r\n",
      "    # Set up a neural network to train.\r\n",
      "    # Classifier reports softmax cross entropy loss and accuracy at every\r\n",
      "    # iteration, which will be used by the PrintReport extension below.\r\n",
      "\r\n",
      "    model = L.Classifier(net.VGG(10))\r\n",
      "    # Make a specified GPU current\r\n",
      "\r\n",
      "    device = comm.intra_rank if num_gpus > 0 else -1\r\n",
      "    if device >= 0:\r\n",
      "        chainer.cuda.get_device_from_id(device).use()\r\n",
      "\r\n",
      "    optimizer = chainermn.create_multi_node_optimizer(chainer.optimizers.MomentumSGD(learning_rate), comm)\r\n",
      "    optimizer.setup(model)\r\n",
      "    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))\r\n",
      "    \r\n",
      "    num_loaders = 2\r\n",
      "    train_iter = chainer.iterators.MultiprocessIterator(train, batch_size, n_processes=num_loaders)\r\n",
      "    test_iter = chainer.iterators.MultiprocessIterator(test, batch_size, repeat=False, n_processes=num_loaders)\r\n",
      "\r\n",
      "    # Set up a trainer\r\n",
      "    updater = training.StandardUpdater(train_iter, optimizer, device=device)\r\n",
      "    trainer = training.Trainer(updater, (epochs, 'epoch'), out=output_data_dir)\r\n",
      "\r\n",
      "    # Evaluate the model with the test dataset for each epoch\r\n",
      "\r\n",
      "    evaluator = extensions.Evaluator(test_iter, model, device=device)\r\n",
      "    evaluator = chainermn.create_multi_node_evaluator(evaluator, comm)\r\n",
      "    trainer.extend(evaluator)\r\n",
      "\r\n",
      "    # Reduce the learning rate by half every 25 epochs.\r\n",
      "    trainer.extend(extensions.ExponentialShift('lr', 0.5), trigger=(25, 'epoch'))\r\n",
      "\r\n",
      "    # Dump a computational graph from 'loss' variable at the first iteration\r\n",
      "    # The \"main\" refers to the target link of the \"main\" optimizer.\r\n",
      "    trainer.extend(extensions.dump_graph('main/loss'))\r\n",
      "\r\n",
      "    # Write a log of evaluation statistics for each epoch\r\n",
      "    trainer.extend(extensions.LogReport())\r\n",
      "    if comm.rank == 0:\r\n",
      "        if extensions.PlotReport.available():\r\n",
      "            trainer.extend(\r\n",
      "                extensions.PlotReport(['main/loss', 'validation/main/loss'],\r\n",
      "                                      'epoch', file_name='loss.png'))\r\n",
      "            trainer.extend(\r\n",
      "                extensions.PlotReport(\r\n",
      "                    ['main/accuracy', 'validation/main/accuracy'],\r\n",
      "                    'epoch', file_name='accuracy.png'))\r\n",
      "\r\n",
      "        trainer.extend(extensions.dump_graph('main/loss'))\r\n",
      "\r\n",
      "        trainer.extend(extensions.PrintReport(\r\n",
      "            ['epoch', 'main/loss', 'validation/main/loss',\r\n",
      "             'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\r\n",
      "        trainer.extend(extensions.ProgressBar())\r\n",
      "\r\n",
      "    # Run the training\r\n",
      "    trainer.run()\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    This function is called by the Chainer container during hosting when running on SageMaker with\r\n",
      "    values populated by the hosting environment.\r\n",
      "    \r\n",
      "    By default, the Chainer container saves models as .npz files, with the name 'model.npz'. In\r\n",
      "    your training script, you can override this behavior by implementing a function with\r\n",
      "    signature `save(model, model_dir)`.\r\n",
      "\r\n",
      "    Args:\r\n",
      "        model_dir (str): path to the directory containing the saved model artifacts\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        a loaded Chainer model\r\n",
      "    \r\n",
      "    For more on `model_fn` and `save`, please visit the sagemaker-python-sdk repository:\r\n",
      "    https://github.com/aws/sagemaker-python-sdk\r\n",
      "    \r\n",
      "    For more on the Chainer container, please visit the sagemaker-chainer-containers repository:\r\n",
      "    https://github.com/aws/sagemaker-chainer-containers\r\n",
      "    \"\"\"\r\n",
      "    chainer.config.train = False\r\n",
      "    model = L.Classifier(net.VGG(10))\r\n",
      "    serializers.load_npz(os.path.join(model_dir, 'model.npz'), model)\r\n",
      "    return model.predictor\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'code/chainer_cifar_vgg_distributed.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training script on SageMaker\n",
    "\n",
    "The ```Chainer``` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on two `ml.p3.2xlarge` instances.\n",
    "\n",
    "This script uses the `chainermn` package, which distributes training with MPI. Your script is run with `mpirun`, so a ChainerMN Communicator object can be used to distribute training. Arguments to `mpirun` are set to sensible defaults, but you can configure how your script is run in distributed mode. See the ```Chainer``` class documentation for more on configuring MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-chainer-2018-05-04-20-04-24-955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................\n",
      "\u001b[32m2018-05-04 20:08:47,957 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:47,958 INFO - root - starting train task\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:47,970 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f01a48dabf8>}\u001b[0m\n",
      "\u001b[32mDownloading s3://sagemaker-us-west-2-038453126632/sagemaker-chainer-2018-05-04-20-04-24-955/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,125 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,213 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-038453126632.s3.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,259 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-038453126632.s3.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,276 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-038453126632.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,344 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-038453126632.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:48,715 INFO - chainer_framework.training - worker node algo-2 is waiting for MPI to start training process \u001b[0m\n",
      "\u001b[32m2018-05-04 20:08:50,717 INFO - chainer_framework.training - MPI started training process on worker node algo-2\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,193 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,194 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,207 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f8fca2f6bf8>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-038453126632/sagemaker-chainer-2018-05-04-20-04-24-955/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,346 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,438 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-038453126632.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,485 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-038453126632.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,503 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-038453126632.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,609 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-038453126632.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:48,945 INFO - chainer_framework.training - hosts that aren't SSHable yet: ['algo-2']\u001b[0m\n",
      "\u001b[31m2018-05-04 20:08:49,948 INFO - chainer_framework.training - mpi_command: mpirun --allow-run-as-root --host algo-1,algo-2 -mca btl_tcp_if_include ethwe -mca oob_tcp_if_include ethwe -mca btl ^openib -x PATH -x LD_LIBRARY_PATH -x LD_PRELOAD=/libchangehostname.so -mca orte_abort_on_non_zero_status 1 -x NCCL_DEBUG=INFO -x NCCL_SOCKET_IFNAME=ethwe -np 2    /mpi_script.sh\u001b[0m\n",
      "\u001b[31mWarning: Permanently added 'algo-2,10.32.0.4' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[31m# Minibatch-size: 256\u001b[0m\n",
      "\u001b[31m# epoch: 20\u001b[0m\n",
      "\u001b[31m# learning rate: 0.05\u001b[0m\n",
      "\u001b[31m# communicator: pure_nccl\u001b[0m\n",
      "\u001b[31mepoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] misc/ibvwrap.cu:60 WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Using internal Network Socket\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] misc/ibvwrap.cu:60 WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO Using internal Network Socket\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO NET : Using interface ethwe:10.40.0.3<0>\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO NET : Using interface ethwe:10.32.0.4<0>\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO NET/Socket : 1 interfaces found\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO NET/Socket : 1 interfaces found\u001b[0m\n",
      "\u001b[31mNCCL version 2.1.15+cuda9.0\u001b[0m\n",
      "\u001b[31mNCCL version 2.1.15+cuda9.0\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO Using 256 threads\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO Min Comp Cap 7\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO NCCL_SINGLE_RING_THRESHOLD=262144\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Using 256 threads\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Min Comp Cap 7\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO NCCL_SINGLE_RING_THRESHOLD=262144\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Using 256 threads\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Min Comp Cap 7\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO NCCL_SINGLE_RING_THRESHOLD=262144\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Ring 00 :    0   1\u001b[0m\n",
      "\u001b[31malgo-2:54:54 [0] INFO 0 -> 1 via NET/Socket/0\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO 1 -> 0 via NET/Socket/0\u001b[0m\n",
      "\u001b[31malgo-1:45:45 [0] INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[31m#033[J     total [#.................................................]  2.56%\u001b[0m\n",
      "\u001b[31mthis epoch [#########################.........................] 51.20%\n",
      "       100 iter, 0 epoch / 20 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J1           1.99539     2.72628               0.229771       0.155127                  75.7818       \u001b[0m\n",
      "\u001b[31m#033[J     total [##................................................]  5.12%\u001b[0m\n",
      "\u001b[31mthis epoch [#.................................................]  2.40%\n",
      "       200 iter, 1 epoch / 20 epochs\n",
      "    3.2576 iters/sec. Estimated time to finish: 0:18:57.712167.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [###...............................................]  7.68%\u001b[0m\n",
      "\u001b[31mthis epoch [##########################........................] 53.60%\n",
      "       300 iter, 1 epoch / 20 epochs\n",
      "    3.3204 iters/sec. Estimated time to finish: 0:18:06.082520.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J2           1.52718     3.85926               0.403506       0.215283                  135.178       \u001b[0m\n",
      "\u001b[31m#033[J     total [#####.............................................] 10.24%\u001b[0m\n",
      "\u001b[31mthis epoch [##................................................]  4.80%\n",
      "       400 iter, 2 epoch / 20 epochs\n",
      "    3.2775 iters/sec. Estimated time to finish: 0:17:49.803903.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [######............................................] 12.80%\u001b[0m\n",
      "\u001b[31mthis epoch [############################......................] 56.00%\n",
      "       500 iter, 2 epoch / 20 epochs\n",
      "    3.2851 iters/sec. Estimated time to finish: 0:17:16.889383.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J3           1.19558     1.38322               0.557212       0.53667                   194.702       \u001b[0m\n",
      "\u001b[31m#033[J     total [#######...........................................] 15.36%\u001b[0m\n",
      "\u001b[31mthis epoch [###...............................................]  7.20%\n",
      "       600 iter, 3 epoch / 20 epochs\n",
      "    3.2771 iters/sec. Estimated time to finish: 0:16:48.879966.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [########..........................................] 17.92%\u001b[0m\n",
      "\u001b[31mthis epoch [#############################.....................] 58.40%\n",
      "       700 iter, 3 epoch / 20 epochs\n",
      "    3.2914 iters/sec. Estimated time to finish: 0:16:14.124580.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J4           0.910426    1.31324               0.674167       0.587695                  254.006       \u001b[0m\n",
      "\u001b[31m#033[J     total [##########........................................] 20.48%\u001b[0m\n",
      "\u001b[31mthis epoch [####..............................................]  9.60%\n",
      "       800 iter, 4 epoch / 20 epochs\n",
      "    3.2839 iters/sec. Estimated time to finish: 0:15:45.915489.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [###########.......................................] 23.04%\u001b[0m\n",
      "\u001b[31mthis epoch [##############################....................] 60.80%\n",
      "       900 iter, 4 epoch / 20 epochs\n",
      "    3.2851 iters/sec. Estimated time to finish: 0:15:15.110728.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J5           0.760488    0.829348              0.734355       0.731201                  314.489       \u001b[0m\n",
      "\u001b[31m#033[J     total [############......................................] 25.60%\u001b[0m\n",
      "\u001b[31mthis epoch [######............................................] 12.00%\n",
      "      1000 iter, 5 epoch / 20 epochs\n",
      "      3.27 iters/sec. Estimated time to finish: 0:14:48.756550.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [##############....................................] 28.16%\u001b[0m\n",
      "\u001b[31mthis epoch [###############################...................] 63.20%\n",
      "      1100 iter, 5 epoch / 20 epochs\n",
      "    3.2912 iters/sec. Estimated time to finish: 0:14:12.646687.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J6           0.652106    1.20308               0.780188       0.633545                  371.493       \u001b[0m\n",
      "\u001b[31m#033[J     total [###############...................................] 30.72%\u001b[0m\n",
      "\u001b[31mthis epoch [#######...........................................] 14.40%\n",
      "      1200 iter, 6 epoch / 20 epochs\n",
      "    3.3003 iters/sec. Estimated time to finish: 0:13:39.991777.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [################..................................] 33.28%\u001b[0m\n",
      "\u001b[31mthis epoch [################################..................] 65.60%\n",
      "      1300 iter, 6 epoch / 20 epochs\n",
      "    3.3055 iters/sec. Estimated time to finish: 0:13:08.470486.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J7           0.581007    0.761859              0.807537       0.764453                  429.706       \u001b[0m\n",
      "\u001b[31m#033[J     total [#################.................................] 35.84%\u001b[0m\n",
      "\u001b[31mthis epoch [########..........................................] 16.80%\n",
      "      1400 iter, 7 epoch / 20 epochs\n",
      "    3.3084 iters/sec. Estimated time to finish: 0:12:37.533798.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [###################...............................] 38.40%\u001b[0m\n",
      "\u001b[31mthis epoch [#################################.................] 68.00%\n",
      "      1500 iter, 7 epoch / 20 epochs\n",
      "    3.3249 iters/sec. Estimated time to finish: 0:12:03.716554.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J8           0.517703    0.759029              0.828205       0.762695                  485.583       \u001b[0m\n",
      "\u001b[31m#033[J     total [####################..............................] 40.96%\u001b[0m\n",
      "\u001b[31mthis epoch [#########.........................................] 19.20%\n",
      "      1600 iter, 8 epoch / 20 epochs\n",
      "    3.3374 iters/sec. Estimated time to finish: 0:11:31.033154.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [#####################.............................] 43.52%\u001b[0m\n",
      "\u001b[31mthis epoch [###################################...............] 70.40%\n",
      "      1700 iter, 8 epoch / 20 epochs\n",
      "    3.3567 iters/sec. Estimated time to finish: 0:10:57.273459.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J9           0.465146    0.602957              0.846554       0.806689                  539.67        \u001b[0m\n",
      "\u001b[31m#033[J     total [#######################...........................] 46.08%\u001b[0m\n",
      "\u001b[31mthis epoch [##########........................................] 21.60%\n",
      "      1800 iter, 9 epoch / 20 epochs\n",
      "    3.3672 iters/sec. Estimated time to finish: 0:10:25.511050.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [########################..........................] 48.64%\u001b[0m\n",
      "\u001b[31mthis epoch [####################################..............] 72.80%\n",
      "      1900 iter, 9 epoch / 20 epochs\n",
      "     3.383 iters/sec. Estimated time to finish: 0:09:53.033740.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J10          0.422325    0.49927               0.862026       0.833105                  594.236       \u001b[0m\n",
      "\u001b[31m#033[J     total [#########################.........................] 51.20%\u001b[0m\n",
      "\u001b[31mthis epoch [############......................................] 24.00%\n",
      "      2000 iter, 10 epoch / 20 epochs\n",
      "    3.3903 iters/sec. Estimated time to finish: 0:09:22.258845.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [##########################........................] 53.76%\u001b[0m\n",
      "\u001b[31mthis epoch [#####################################.............] 75.20%\n",
      "      2100 iter, 10 epoch / 20 epochs\n",
      "    3.4031 iters/sec. Estimated time to finish: 0:08:50.773431.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J11          0.389324    0.6705                0.874239       0.785986                  648.479       \u001b[0m\n",
      "\u001b[31m#033[J     total [############################......................] 56.32%\u001b[0m\n",
      "\u001b[31mthis epoch [#############.....................................] 26.40%\n",
      "      2200 iter, 11 epoch / 20 epochs\n",
      "    3.4079 iters/sec. Estimated time to finish: 0:08:20.681939.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [#############################.....................] 58.88%\u001b[0m\n",
      "\u001b[31mthis epoch [######################################............] 77.60%\n",
      "      2300 iter, 11 epoch / 20 epochs\n",
      "    3.4174 iters/sec. Estimated time to finish: 0:07:50.024132.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J12          0.360364    0.562877              0.880849       0.82373                   703.194       \u001b[0m\n",
      "\u001b[31m#033[J     total [##############################....................] 61.44%\u001b[0m\n",
      "\u001b[31mthis epoch [##############....................................] 28.80%\n",
      "      2400 iter, 12 epoch / 20 epochs\n",
      "    3.4196 iters/sec. Estimated time to finish: 0:07:20.469263.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [################################..................] 64.00%\u001b[0m\n",
      "\u001b[31mthis epoch [########################################..........] 80.00%\n",
      "      2500 iter, 12 epoch / 20 epochs\n",
      "    3.4285 iters/sec. Estimated time to finish: 0:06:50.160688.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J13          0.334093    0.589376              0.890386       0.820312                  758.453       \u001b[0m\n",
      "\u001b[31m#033[J     total [#################################.................] 66.56%\u001b[0m\n",
      "\u001b[31mthis epoch [###############...................................] 31.20%\n",
      "      2600 iter, 13 epoch / 20 epochs\n",
      "    3.4309 iters/sec. Estimated time to finish: 0:06:20.725548.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [##################################................] 69.12%\u001b[0m\n",
      "\u001b[31mthis epoch [#########################################.........] 82.40%\n",
      "      2700 iter, 13 epoch / 20 epochs\n",
      "    3.4394 iters/sec. Estimated time to finish: 0:05:50.719803.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J14          0.307406    0.589556              0.899319       0.817529                  812.888       \u001b[0m\n",
      "\u001b[31m#033[J     total [###################################...............] 71.68%\u001b[0m\n",
      "\u001b[31mthis epoch [################..................................] 33.60%\n",
      "      2800 iter, 14 epoch / 20 epochs\n",
      "    3.4423 iters/sec. Estimated time to finish: 0:05:21.369807.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [#####################################.............] 74.24%\u001b[0m\n",
      "\u001b[31mthis epoch [##########################################........] 84.80%\n",
      "      2900 iter, 14 epoch / 20 epochs\n",
      "    3.4496 iters/sec. Estimated time to finish: 0:04:51.701923.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J15          0.28805     0.587012              0.905329       0.82251                   867.208       \u001b[0m\n",
      "\u001b[31m#033[J     total [######################################............] 76.80%\u001b[0m\n",
      "\u001b[31mthis epoch [#################.................................] 36.00%\n",
      "      3000 iter, 15 epoch / 20 epochs\n",
      "    3.4525 iters/sec. Estimated time to finish: 0:04:22.491115.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [#######################################...........] 79.36%\u001b[0m\n",
      "\u001b[31mthis epoch [###########################################.......] 87.20%\n",
      "      3100 iter, 15 epoch / 20 epochs\n",
      "    3.4582 iters/sec. Estimated time to finish: 0:03:53.140475.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J16          0.272646    0.515968              0.909956       0.844336                  921.79        \u001b[0m\n",
      "\u001b[31m#033[J     total [########################################..........] 81.92%\u001b[0m\n",
      "\u001b[31mthis epoch [###################...............................] 38.40%\n",
      "      3200 iter, 16 epoch / 20 epochs\n",
      "    3.4584 iters/sec. Estimated time to finish: 0:03:24.215454.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [##########################################........] 84.48%\u001b[0m\n",
      "\u001b[31mthis epoch [############################################......] 89.60%\n",
      "      3300 iter, 16 epoch / 20 epochs\n",
      "     3.457 iters/sec. Estimated time to finish: 0:02:55.370162.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J17          0.25814     0.509383              0.915637       0.844141                  979.271       \u001b[0m\n",
      "\u001b[31m#033[J     total [###########################################.......] 87.04%\u001b[0m\n",
      "\u001b[31mthis epoch [####################..............................] 40.80%\n",
      "      3400 iter, 17 epoch / 20 epochs\n",
      "    3.4514 iters/sec. Estimated time to finish: 0:02:26.678684.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [############################################......] 89.60%\u001b[0m\n",
      "\u001b[31mthis epoch [##############################################....] 92.00%\n",
      "      3500 iter, 17 epoch / 20 epochs\n",
      "    3.4508 iters/sec. Estimated time to finish: 0:01:57.726540.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J18          0.245145    0.656397              0.920092       0.813867                  1037.38       \u001b[0m\n",
      "\u001b[31m#033[J     total [##############################################....] 92.16%\u001b[0m\n",
      "\u001b[31mthis epoch [#####################.............................] 43.20%\n",
      "      3600 iter, 18 epoch / 20 epochs\n",
      "    3.4471 iters/sec. Estimated time to finish: 0:01:28.842322.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [###############################################...] 94.72%\u001b[0m\n",
      "\u001b[31mthis epoch [###############################################...] 94.40%\n",
      "      3700 iter, 18 epoch / 20 epochs\n",
      "    3.4472 iters/sec. Estimated time to finish: 0:00:59.831072.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J19          0.232822    0.470814              0.924379       0.857861                  1094.99       \u001b[0m\n",
      "\u001b[31m#033[J     total [################################################..] 97.28%\u001b[0m\n",
      "\u001b[31mthis epoch [######################............................] 45.60%\n",
      "      3800 iter, 19 epoch / 20 epochs\n",
      "    3.4431 iters/sec. Estimated time to finish: 0:00:30.859023.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J     total [#################################################.] 99.84%\u001b[0m\n",
      "\u001b[31mthis epoch [################################################..] 96.80%\n",
      "      3900 iter, 19 epoch / 20 epochs\n",
      "    3.4424 iters/sec. Estimated time to finish: 0:00:01.815589.\u001b[0m\n",
      "\u001b[31m#033[4A#033[J20          0.218399    0.508409              0.929209       0.847559                  1153.34       \u001b[0m\n",
      "\u001b[31m#033[J\u001b[0m\n",
      "\u001b[32m2018-05-04 20:28:11,613 INFO - chainer_framework.training - Training process started by MPI on worker node algo-2 stopped\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n",
      "Billable seconds: 2700\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='chainer_cifar_vgg_distributed.py', source_dir=\"code\", role=role,\n",
    "                            use_mpi=True, sagemaker_session=sagemaker_session,\n",
    "                            train_instance_count=2, train_instance_type='ml.p3.2xlarge',\n",
    "                            hyperparameters={'epochs': 30, 'batch_size': 256})\n",
    "\n",
    "chainer_estimator.fit({'train': train_input, 'test': test_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Chainer script writes various artifacts, such as plots, to a directory `output_data_dir`, the contents of which which SageMaker uploads to S3. Now we download and extract these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_util import retrieve_output_from_s3\n",
    "\n",
    "chainer_training_job = chainer_estimator.latest_training_job.name\n",
    "\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=chainer_training_job)\n",
    "output_data = desc['ModelArtifacts']['S3ModelArtifacts'].replace('model.tar.gz', 'output.tar.gz')\n",
    "\n",
    "retrieve_output_from_s3(output_data, 'output/distributed_cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "These plots show the accuracy and loss over epochs:\n",
       "\n",
       "<img style=\"display: inline;\" src=\"output/distributed_cifar/algo-1/accuracy.png?1525465777.5663912\" />\n",
       "<img style=\"display: inline;\" src=\"output/distributed_cifar/algo-1/loss.png?1525465777.5663912\" />"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executing as code to reload images so that browsers don't render cached images.\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "_nonce = time.time()\n",
    "\n",
    "Markdown(\"\"\"\n",
    "These plots show the accuracy and loss over epochs:\n",
    "\n",
    "<img style=\"display: inline;\" src=\"output/distributed_cifar/algo-1/accuracy.png?{0}\" />\n",
    "<img style=\"display: inline;\" src=\"output/distributed_cifar/algo-1/loss.png?{0}\" />\"\"\".format(_nonce))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Trained Model\n",
    "\n",
    "After training, we use the Chainer estimator object to create and deploy a hosted prediction endpoint. We can use a CPU-based instance for inference (in this case an `ml.m4.xlarge`), even though we trained on GPU instances.\n",
    "\n",
    "The predictor object returned by `deploy` lets us call the new endpoint and perform inference on our sample images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-chainer-2018-05-04-20-04-24-955\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-chainer-2018-05-04-20-04-24-955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 sample images\n",
    "\n",
    "We'll use these CIFAR10 sample images to test the service:\n",
    "\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/airplane1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/automobile1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/bird1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/cat1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/deer1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/dog1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/frog1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/horse1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/ship1.png\" />\n",
    "<img style=\"display: inline; height: 32px; margin: 0.25em\" src=\"images/truck1.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using SageMaker Endpoint\n",
    "\n",
    "We batch the images together into a single NumPy array to obtain multiple inferences with a single prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "def read_image(filename):\n",
    "    img = io.imread(filename)\n",
    "    img = np.array(img).transpose(2, 0, 1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img.astype(np.float32)\n",
    "    img *= 1. / 255.\n",
    "    img = img.reshape(3, 32, 32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_images(filenames):\n",
    "    return np.array([read_image(f) for f in filenames])\n",
    "\n",
    "filenames = ['images/airplane1.png',\n",
    "             'images/automobile1.png',\n",
    "             'images/bird1.png',\n",
    "             'images/cat1.png',\n",
    "             'images/deer1.png',\n",
    "             'images/dog1.png',\n",
    "             'images/frog1.png',\n",
    "             'images/horse1.png',\n",
    "             'images/ship1.png',\n",
    "             'images/truck1.png']\n",
    "\n",
    "image_data = read_images(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor runs inference on our input data and returns a list of predictions whose argmax gives the predicted label of the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0: prediction: 0\n",
      "image 1: prediction: 1\n",
      "image 2: prediction: 2\n",
      "image 3: prediction: 3\n",
      "image 4: prediction: 4\n",
      "image 5: prediction: 5\n",
      "image 6: prediction: 6\n",
      "image 7: prediction: 7\n",
      "image 8: prediction: 8\n",
      "image 9: prediction: 9\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(image_data)\n",
    "\n",
    "for i, prediction in enumerate(response):\n",
    "    print('image {}: prediction: {}'.format(i, prediction.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-chainer-2018-05-04-20-04-24-955\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
