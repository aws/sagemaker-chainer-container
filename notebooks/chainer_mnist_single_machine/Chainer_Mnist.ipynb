{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Training and Prediction with SageMaker Chainer\n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/), the \"Hello World\" of machine learning, is a popular dataset for handwritten digit classification. It consists of 70,000 28x28 grayscale images labeled in 10 digit classes(0 to 9). This tutorial will show how to train on MNIST using SageMaker Chainer in [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST datasets\n",
    "\n",
    "We can use chainer built-in get_mnist() method to download, import and structure MNIST dataset. It may take several minutes to run this cell for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "\n",
    "try:\n",
    "    os.makedirs('data/train')\n",
    "    os.makedirs('data/test')\n",
    "except FileExistsError:\n",
    "    print('Directories already exist!')\n",
    "\n",
    "train, test = chainer.datasets.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse, save, and upload the data\n",
    "\n",
    "Now we split each of training dataset and testing dataset to images and labels. Then it's convenient for us to save training data and testing data to npz files, a format that's popular in chainer world, to local filesystem. Then we can use `sagemaker.Session.upload_data` to upload the data to an S3 location used for training. The return value `inputs` identifies this S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_images = np.array([data[0] for data in train])\n",
    "train_labels = np.array([data[1] for data in train])\n",
    "test_images = np.array([data[0] for data in test])\n",
    "test_labels = np.array([data[1] for data in test])\n",
    "\n",
    "np.savez('data/train/train.npz', images=train_images, labels=train_labels)\n",
    "np.savez('data/test/test.npz', images=test_images, labels=test_labels)\n",
    "\n",
    "train_input = sagemaker_session.upload_data(path=os.path.join('data', 'train'), key_prefix='notebook/chainer/mnist')\n",
    "test_input = sagemaker_session.upload_data(path=os.path.join('data', 'test'), key_prefix='notebook/chainer/mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare user script for training and prediction\n",
    "\n",
    "We need to provide a user script that can run on the SageMaker platform. The script is essentially the same as one you would write for local training, except that a function `train` that returns a trained `chainer.Chain` model is required. By default, this model is saved to disk as an npz file named `model.npz`.\n",
    "For prediciton, this script also requires a function `model_fn` that loads the `chainer.Chain` saved from training. When SageMaker calls your `train` and `model_fn` functions, it will pass in arguments that describe the training environment.\n",
    "\n",
    "### TODO: Add link to README for user script writing here.\n",
    "\n",
    "The below user script used in our tutorial is adapted from [chainer mnist example](https://github.com/chainer/chainer/blob/master/examples/mnist/train_mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'chainer_mnist_single_machine.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker chainer estimator\n",
    "\n",
    "To train MNIST, let's construct a `sagemaker.chainer.estimator.Chainer` estimator. A quick explanation for some configurable arguments here:\n",
    "\n",
    "`entry_point`: The user script SageMaker runs for training and prediction.\n",
    "\n",
    "`train_instance_count`: The number of SageMaker instances for training. Since we only do single machine training in this tutorial, it should be 1.\n",
    "\n",
    "`train_instance_type`: The type of SageMaker instances for training. We choose `ml.c5.xlarge` for cpu training. If you want gpu training, you could choose other instance type accordingly. See [Amazon SageMaker ML Instance Types](https://aws.amazon.com/sagemaker/pricing/instance-types/)\n",
    "\n",
    "`hyperparameters`: The hyper-parameters defined in the user script. In this tutorial, `epochs`, `batch_size` and `frequency` can be configured and passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='chainer_mnist_single_machine.py', role=role,\n",
    "                            sagemaker_session=sagemaker_session,\n",
    "                            train_instance_count=1, train_instance_type='ml.c5.xlarge',\n",
    "                            hyperparameters={'epochs': 16, 'batch_size': 128})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on MNIST data in S3\n",
    "\n",
    "After we've constructed our Chainer object, we can fit it using the MNIST data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our user script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.fit({'train': train_input, 'test': test_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy model to endpoint\n",
    "\n",
    "After training, we deploy the model to an endpoint using the Chainer estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Hand-Written Digit\n",
    "\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a variable named 'data' in this notebook, which we can then pass to the Chainer predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if your writing can be recognized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(data, dtype=np.float32)\n",
    "prediction = predictor.predict(image)\n",
    "predicted_label = prediction.argmax(axis=1)[0]\n",
    "print('What you wrote is: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some random test images in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "indices = random.sample(range(test_images.shape[0] - 1), num_samples)\n",
    "images, labels = test_images[indices], test_labels[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can make correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(images)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('The predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
