{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentiment analysis model with Chainer\n",
    "\n",
    "In this notebook, we will train a model that will allow us to analyze text for positive or negative sentiment. The model will use a recurrent neural network with long short-term memory blocks to generate word embeddings.\n",
    "\n",
    "The Chainer script runs inside of a Docker container running on SageMaker. For more on the Chainer container, please visit the sagemaker-chainer-containers repository and the sagemaker-python-sdk repository:\n",
    "\n",
    "* https://github.com/aws/sagemaker-chainer-containers\n",
    "* https://github.com/aws/sagemaker-python-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This role retrieves the SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading training and test data\n",
    "\n",
    "We use helper functions given by `chainer` to download and preprocess the data. We'll be using the Stanford Sentiment Treebank dataset, which consists of sentence fragments along with labels indicating whether the sentence is has a positive sentiment (1) or negative sentiment (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "file_paths = dataset.download_dataset(\"stsa.binary\")\n",
    "\n",
    "new_file_paths = dataset.get_stsa_dataset(file_paths)\n",
    "train, test, vocab = dataset.get_stsa_dataset(file_paths)\n",
    "\n",
    "with open(file_paths[0], 'r') as f:\n",
    "    for i in range(20):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We save the preprocessed data to the local filesystem, and then use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the S3 location, which we will use when we start the Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data = [element[0] for element in train]\n",
    "train_labels = [element[1] for element in train]\n",
    "\n",
    "test_data = [element[0] for element in test]\n",
    "test_labels = [element[1] for element in test]\n",
    "\n",
    "\n",
    "try:\n",
    "    os.makedirs('/tmp/data/train')\n",
    "    os.makedirs('/tmp/data/test')\n",
    "    os.makedirs('/tmp/data/vocab')\n",
    "    np.savez('/tmp/data/train/train.npz', data=train_data, labels=train_labels)\n",
    "    np.savez('/tmp/data/test/test.npz', data=test_data, labels=test_labels)\n",
    "    np.save('/tmp/data/vocab/vocab.npy', vocab)\n",
    "    train_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'train'),\n",
    "                                                            key_prefix='notebook/chainer_cifar/train')\n",
    "    test_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'test'),\n",
    "                                                           key_prefix='notebook/chainer_cifar/test')\n",
    "    vocab_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'vocab'),\n",
    "                                                           key_prefix='notebook/chainer_sentiment/vocab')\n",
    "finally:\n",
    "    shutil.rmtree('/tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Chainer training script to run on Amazon SageMaker\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a function `train` that returns a trained `chainer.Chain`.\n",
    "\n",
    "Since we will use the same script to host the Chainer model, the script also needs a function `model_fn` that loads a `chainer.Chain` -- by default, Chainer models are saved to disk as `model.npz`. When SageMaker calls your `train` and `model_fn` functions, it will pass in arguments that describe the training environment.\n",
    "\n",
    "While the `train` and `model_fn` functions are required, the Chainer container provides default implementations for a few other functions. The function hooks recognized by the container are listed below, with required functions in bold:\n",
    "\n",
    "### Training\n",
    "\n",
    "* **`train`**: This function is passed arguments read from the Training Job's environment and returns a trained model. The return value of `train` is saved and uploaded to S3 as a model artifact by `save`.\n",
    "\n",
    "  `train` can accept the following arguments by name:\n",
    "  * `hyperparameters (dict)`: The hyperparameters map passed from the SageMaker Python SDK.\n",
    "  * `channel_input_dirs (dict of str: str)`: A map of input channel names (like 'train' and 'test') to filesystem paths to data in those input channels. \n",
    "  * `output_data_dir (str)`: The filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files you might like to save, not including model artifacts. These artifacts are uploaded to S3 along with your model artifacts.\n",
    "  * `num_gpus (int): ` The number of GPUs available to the host.\n",
    "  * `num_cpus (int): `: The number of CPUs available to the host.\n",
    "  * `hosts (list of str)`: The list of hostnames for all Training Job instances.\n",
    "  * `current_host (str)`: The hostname of the current host.\n",
    "  \n",
    "  For more on the arguments to `train` and others, please visit https://github.com/aws/sagemaker-containers.\n",
    "  \n",
    "  \n",
    "* `save(model, model_dir)`: Writes the return value from `train` (`model`) to `model_dir`. These model artifacts are uploaded to S3 so that they can be hosted behind a SageMaker Endpoint.\n",
    "\n",
    "  The default implementation saves the `model` as a file named `model.npz` file by invoking `chainer.serializers.save_npz`\n",
    "\n",
    "### Hosting and Inference\n",
    "\n",
    "* **`model_fn(model_dir)`**: This function is invoked to load model artifacts from those written into `model_dir` by `save`.input_data\n",
    "* `input_fn(input_data, content_type)`: This function is invoked to deserialize prediction data when a prediction request is made. The return value is passed to predict_fn. `input_fn` accepts two arguments: `input_data`, which is the serialized input data in the body of the prediction request, and `content_type`, the MIME type of the data.\n",
    "  \n",
    "  The default implementation deserializes [npy-formatted](https://docs.scipy.org/doc/numpy-1.14.0/neps/npy-format.html) data into a NumPy array with content type 'application/x-npy', but the default handler can also handle CSV data with content type 'text/csv' and JSON data with content type 'application/json'\n",
    "  \n",
    "  `input_fn` accepts the following arguments:\n",
    "  \n",
    "  * `input_data`: serialized input data in the body of the prediction request.\n",
    "  * `content_type`: MIME type of the data. By default, the Chainer predictor sends prediction requests with content type 'application/x-npy'.\n",
    "  \n",
    "  \n",
    "* `predict_fn(input_data, model)`: This function accepts the return value of `input_fn` (as `input_data`) and the return value of `model_fn`, `model`, and returns inferences obtained from the model.\n",
    "\n",
    "  The default implementation calls `model(input_data)` and returns the result as a NumPy array.\n",
    "  \n",
    "  \n",
    "* `output_fn(prediction, accept)`: This function is invoked to serialize the return value from `predict_fn`, passed in via `prediction`, back to the SageMaker client in response to prediction requests\n",
    "\n",
    "  The default implementation serializes NumPy arrays returned by `predict_fn`, which the SageMaker Python SDK can deserialize back into a NumPy array, but the default handler can also respond with JSON or CSV, depending on the `accept` MIME type given in the prediction request.\n",
    "\n",
    "Check the script below, which uses `chainer` to train on any number of GPUs on a single machine, to see how this works. This script implements `train`, `save`, `model_fn`, and `predict_fn`, but relies on the default `input_fn` and `output_fn`.\n",
    "\n",
    "For more on implementing these functions, see the documentation at https://github.com/aws/sagemaker-python-sdk.\n",
    "\n",
    "For more on the functions provided by the Chainer container, see https://github.com/aws/sagemaker-chainer-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat 'code/sentiment_analysis.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training script on SageMaker\n",
    "\n",
    "To train with a Chainer script, we construct a ```Chainer``` estimator using the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk). We can pass in an `entry_point`, the name of a script that contains a couple of functions with certain signatures (`train` and `model_fn`). This script will be run on SageMaker in a container that invokes these functions to train and load Chainer models.\n",
    "\n",
    "The ```Chainer``` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on a `ml.p3.2xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='sentiment_analysis.py', source_dir=\"code\", role=role,\n",
    "                            sagemaker_session=sagemaker_session,\n",
    "                            train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n",
    "                            hyperparameters={'epochs': 10, 'batch_size': 64})\n",
    "\n",
    "chainer_estimator.fit({'train': train_input, 'test': test_input, 'vocab': vocab_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Chainer script writes various artifacts, such as plots, to a directory `output_data_dir`, the contents of which which SageMaker uploads to S3. Now we download and extract these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_util import retrieve_output_from_s3\n",
    "\n",
    "chainer_training_job = chainer_estimator.latest_training_job.name\n",
    "\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=chainer_training_job)\n",
    "output_data = desc['ModelArtifacts']['S3ModelArtifacts'].replace('model.tar.gz', 'output.tar.gz')\n",
    "\n",
    "retrieve_output_from_s3(output_data, 'output/sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the accuracy and loss over epochs.\n",
    "\n",
    "In our user script, `sentiment_analysis.py`, at the end of the `train` function, we save only the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing as code to reload images so that browsers don't render cached images.\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "_nonce = time.time()\n",
    "\n",
    "Markdown(\"\"\"\n",
    "These plots show the accuracy and loss over epochs.\n",
    "\n",
    "In our user script (sentiment_analysis.py), we save only the best model for deployment.\n",
    "\n",
    "<img style=\"display: inline;\" src=\"output/sentiment/accuracy.png?{0}\" />\n",
    "<img style=\"display: inline;\" src=\"output/sentiment/loss.png?{0}\" />\"\"\".format(_nonce))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Trained Model\n",
    "\n",
    "After training, we use the Chainer estimator object to create and deploy a hosted prediction endpoint. We can use a CPU-based instance for inference (in this case an `ml.m4.xlarge`), even though we trained on GPU instances.\n",
    "\n",
    "The predictor object returned by `deploy` lets us call the new endpoint and perform inference on our sample images.\n",
    "\n",
    "At the end of training, `sentiment_analysis.py` saves the trained model, the vocabulary, and a dictionary of model properties that are used to reconstruct the model. These model artifacts are loaded in `model_fn` when the model is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using SageMaker Endpoint\n",
    "\n",
    "The Chainer predictor converts its input into a NumPy array, which it serializes and sends to the hosted model.\n",
    "The `predict_fn` in `sentiment_analysis.py` receives this NumPy array and uses the loaded model to make predictions on the input data, which it returns as a NumPy array back to the Chainer predictor.\n",
    "\n",
    "We predict against the hosted model on a batch of sentences. The output, as defined by `predict_fn`, consists of the processed input sentence, the prediction, and the score for that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['It is fun and easy to train Chainer models on Amazon SageMaker!',\n",
    "             'It used to be slow, difficult, and laborious to train and deploy a model to production.',\n",
    "             'But now it is super fast to deploy to production. And I love it when my model generalizes!',]\n",
    "predictions = predictor.predict(sentences)\n",
    "for prediction in predictions:\n",
    "    sentence, prediction, score = prediction\n",
    "    print('sentence: {}\\nprediction: {}\\nscore: {}\\n'.format(sentence, prediction, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict against sentences in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_paths[1], 'r') as f:\n",
    "    sentences = f.readlines(2000)\n",
    "    sentences = [sentence[1:].strip() for sentence in sentences]\n",
    "    predictions = predictor.predict(sentences)\n",
    "\n",
    "predictions = predictor.predict(sentences)\n",
    "\n",
    "for prediction in predictions:\n",
    "    sentence, prediction, score = prediction\n",
    "    print('sentence: {}\\nprediction: {}\\nscore: {}\\n'.format(sentence, prediction, score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
