{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentiment analysis model with Chainer\n",
    "\n",
    "In this notebook, we will train a model that will allow us to analyze text for positive or negative sentiment. The model will use a recurrent neural network with long short-term memory blocks to generate word embeddings.\n",
    "\n",
    "For more on the Chainer container, please visit the sagemaker-chainer-containers repository and the sagemaker-python-sdk repository:\n",
    "\n",
    "* https://github.com/aws/sagemaker-chainer-containers\n",
    "* https://github.com/aws/sagemaker-python-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This role retrieves the SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading training and test data\n",
    "\n",
    "We use helper functions given by `chainer` to download and preprocess the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "train, test, vocab = dataset.get_stsa_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We save the preprocessed data to the local filesystem, and then use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the S3 location, which we will use when we start the Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data = [element[0] for element in train]\n",
    "train_labels = [element[1] for element in train] \n",
    "\n",
    "test_data = [element[0] for element in test]\n",
    "test_labels = [element[1] for element in test]\n",
    "\n",
    "try:\n",
    "    os.makedirs('data/train')\n",
    "    os.makedirs('data/test')\n",
    "    os.makedirs('data/vocab')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "np.savez('data/train/train.npz', data=train_data, labels=train_labels)\n",
    "np.savez('data/test/test.npz', data=test_data, labels=test_labels)\n",
    "np.save('data/vocab/vocab.npy', vocab)\n",
    "\n",
    "# Upload preprocessed data to S3 \n",
    "train_input = sagemaker_session.upload_data(path=os.path.join('data', 'train'),\n",
    "                                                            key_prefix='notebook/chainer_sentiment/train')\n",
    "test_input = sagemaker_session.upload_data(path=os.path.join('data', 'test'),\n",
    "                                                           key_prefix='notebook/chainer_sentiment/test')\n",
    "vocab_input = sagemaker_session.upload_data(path=os.path.join('data', 'vocab'),\n",
    "                                                           key_prefix='notebook/chainer_sentiment/vocab')\n",
    "\n",
    "# Remove data from notebook instance (to save disk space)\n",
    "shutil.rmtree('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Chainer training script to run on Amazon SageMaker\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a function `train` that returns a trained `chainer.Chain`. Since we will use the same script to host the Chainer model, the script also needs a function `model_fn` that loads a `chainer.Chain` -- by default, Chainer models are saved to disk as `model.npz`. When SageMaker calls your `train` and `model_fn` functions, it will pass in arguments that describe the training environment.\n",
    "\n",
    "Check the script below, which uses `chainer` to train on any number of GPUs on a single machine, to see how this works. For more on implementing these functions, see the documentation at https://github.com/aws/sagemaker-python-sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat 'code/sentiment_analysis.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training script on SageMaker\n",
    "\n",
    "To train with a Chainer script, we construct a ```Chainer``` estimator using the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk). We can pass in an `entry_point`, the name of a script that contains a couple of functions with certain signatures (`train` and `model_fn`). This script will be run on SageMaker in a container that invokes these functions to train and load Chainer models. \n",
    "\n",
    "The ```Chainer``` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on two `ml.p3.2xlarge` instances.\n",
    "\n",
    "Chainer scripts can distribute training with the `chainermn` package, which this Chainer script does not use, so this script should only be run on one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='sentiment_analysis.py', source_dir=\"code\", role=role,\n",
    "                            sagemaker_session=sagemaker_session,\n",
    "                            train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n",
    "                            hyperparameters={'epochs': 10, 'batch_size': 64})\n",
    "\n",
    "chainer_estimator.fit({'train': train_input, 'test': test_input, 'vocab': vocab_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Chainer script writes various artifacts, such as plots, to a directory `output_data_dir`, the contents of which which SageMaker uploads to S3. Now we download and extract these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_util import retrieve_output_from_s3\n",
    "\n",
    "chainer_training_job = chainer_estimator.latest_training_job.name\n",
    "\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=chainer_training_job)\n",
    "output_data = desc['ModelArtifacts']['S3ModelArtifacts'].replace('model.tar.gz', 'output.tar.gz')\n",
    "\n",
    "retrieve_output_from_s3(output_data, 'output/sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the accuracy and loss over epochs.\n",
    "\n",
    "In our user script, `sentiment_analysis.py`, at the end of the `train` function, we save only the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing as code to reload images so that browsers don't render cached images.\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "_nonce = time.time()\n",
    "\n",
    "Markdown(\"\"\"\n",
    "These plots show the accuracy and loss over epochs.\n",
    "\n",
    "In our user script (sentiment_analysis.py), we save only the best model for deployment.\n",
    "\n",
    "<img style=\"display: inline;\" src=\"output/sentiment/accuracy.png?{0}\" />\n",
    "<img style=\"display: inline;\" src=\"output/sentiment/loss.png?{0}\" />\"\"\".format(_nonce))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Trained Model\n",
    "\n",
    "After training, we use the Chainer estimator object to create and deploy a hosted prediction endpoint. We can use a CPU-based instance for inference (in this case an `ml.m4.xlarge`), even though we trained on GPU instances.\n",
    "\n",
    "The predictor object returned by `deploy` lets us call the new endpoint and perform inference on our sample images.\n",
    "\n",
    "At the end of training, `sentiment_analysis.py` saves the trained model, the vocabulary, and a dictionary of model properties that are used to reconstruct the model. These model artifacts are loaded in `model_fn` when the model is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using SageMaker Endpoint\n",
    "\n",
    "The Chainer predictor converts its input into a NumPy array, which it serializes and sends to the hosted model.\n",
    "The `predict_fn` in `sentiment_analysis.py` receives this NumPy array and uses the loaded model to make predictions on the input data, which it returns as a NumPy array back to the Chainer predictor.\n",
    "\n",
    "We predict against the hosted model on a batch of sentences. The output, as defined by `predict_fn`, consists of the processed input sentence, the prediction, and the score for that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['It is fun and easy to train Chainer models on Amazon SageMaker!',\n",
    "             'It used to be slow, difficult, and laborious to train and deploy a model to production.',\n",
    "             'But now it is super fast to deploy to production. And I love it when my model generalizes!',]\n",
    "predictions = predictor.predict(sentences)\n",
    "for prediction in predictions:\n",
    "    sentence, prediction, score = prediction\n",
    "    print('sentence: {}\\nprediction: {}\\nscore: {}\\n'.format(sentence, prediction, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
