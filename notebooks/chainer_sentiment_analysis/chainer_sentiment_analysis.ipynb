{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentiment analysis model with Chainer\n",
    "\n",
    "In this notebook, we will train a model that will allow us to analyze text for positive or negative sentiment. The model will use a recurrent neural network with long short-term memory blocks to generate word embeddings.\n",
    "\n",
    "The Chainer script runs inside of a Docker container running on SageMaker. For more on the Chainer container, please visit the sagemaker-chainer-containers repository and the sagemaker-python-sdk repository:\n",
    "\n",
    "* https://github.com/aws/sagemaker-chainer-containers\n",
    "* https://github.com/aws/sagemaker-python-sdk\n",
    "\n",
    "For more on Chainer, please visit the Chainer repository:\n",
    "\n",
    "* https://github.com/chainer/chainer\n",
    "\n",
    "The code in this notebook is adapted from the [text classification](https://github.com/chainer/chainer/tree/master/examples/text_classification) example in the Chainer repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This role retrieves the SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading training and test data\n",
    "\n",
    "We use helper functions given by `chainer` to download and preprocess the data. We'll be using the [Stanford Sentiment Treebank dataset](https://nlp.stanford.edu/sentiment/), which consists of sentence fragments from movie reviews along with labels indicating whether the sentence has a positive sentiment (1) or negative sentiment (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "file_paths = dataset.download_dataset(\"stsa.binary\")\n",
    "\n",
    "new_file_paths = dataset.get_stsa_dataset(file_paths)\n",
    "train, test, vocab = dataset.get_stsa_dataset(file_paths)\n",
    "\n",
    "with open(file_paths[0], 'r') as f:\n",
    "    for i in range(20):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We save the preprocessed data to the local filesystem, and then use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the S3 location, which we will use when we start the Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data = [element[0] for element in train]\n",
    "train_labels = [element[1] for element in train]\n",
    "\n",
    "test_data = [element[0] for element in test]\n",
    "test_labels = [element[1] for element in test]\n",
    "\n",
    "\n",
    "try:\n",
    "    os.makedirs('/tmp/data/train_sentiment')\n",
    "    os.makedirs('/tmp/data/test_sentiment')\n",
    "    os.makedirs('/tmp/data/vocab')\n",
    "    np.savez('/tmp/data/train_sentiment/train.npz', data=train_data, labels=train_labels)\n",
    "    np.savez('/tmp/data/test_sentiment/test.npz', data=test_data, labels=test_labels)\n",
    "    np.save('/tmp/data/vocab/vocab.npy', vocab)\n",
    "    train_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'train_sentiment'),\n",
    "                                                            key_prefix='notebook/chainer_sentiment/train')\n",
    "    test_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'test_sentiment'),\n",
    "                                                           key_prefix='notebook/chainer_sentiment/test')\n",
    "    vocab_input = sagemaker_session.upload_data(path=os.path.join('/tmp', 'data', 'vocab'),\n",
    "                                                           key_prefix='notebook/chainer_sentiment/vocab')\n",
    "finally:\n",
    "    shutil.rmtree('/tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Chainer training script to run on Amazon SageMaker\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a function `train` that returns a trained `chainer.Chain`.\n",
    "\n",
    "Since we will use the same script to host the Chainer model, the script also needs a function `model_fn` that loads a Chainer Model. When SageMaker calls your `train` and `model_fn` functions, it will pass in arguments that describe the training environment.\n",
    "\n",
    "While the `train` and `model_fn` functions are required, the Chainer container provides default implementations for a few other functions. The function hooks recognized by the container are listed below, with required functions in bold:\n",
    "\n",
    "### Training\n",
    "\n",
    "* **`train`**: This function is passed arguments read from the Training Job's environment and returns a trained model. Files written to `model_dir` are compressed and uploaded to S3 at the end of training.\n",
    "  `train` can accept the following arguments by name:\n",
    "  * `hyperparameters (dict)`: The hyperparameters map passed from the SageMaker Python SDK.\n",
    "  * `num_gpus (int): ` The number of GPUs available to the host.\n",
    "  * `output_data_dir (str)`: The filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3.\n",
    "  * `channel_input_dirs (dict of str: str)`: A map of input channel names (like 'train' and 'test') to filesystem paths to data in those input channels. \n",
    "  * `model_dir (str)`: path to the directory to write model artifacts to.\n",
    "  \n",
    "  For more on the arguments to `train` and others, please visit https://github.com/aws/sagemaker-containers.\n",
    "  \n",
    "\n",
    "### Hosting and Inference\n",
    "\n",
    "* **`model_fn(model_dir)`**: This function is invoked to load model artifacts from those written into `model_dir` during training.\n",
    "* `input_fn(input_data, content_type)`: This function is invoked to deserialize prediction data when a prediction request is made. The return value is passed to predict_fn. `input_fn` accepts two arguments: `input_data`, which is the serialized input data in the body of the prediction request, and `content_type`, the MIME type of the data\n",
    "  \n",
    "  \n",
    "* `predict_fn(input_data, model)`: This function accepts the return value of `input_fn` (as `input_data`) and the return value of `model_fn`, `model`, and returns inferences obtained from the model\n",
    "  \n",
    "  \n",
    "* `output_fn(prediction, accept)`: This function is invoked to serialize the return value from `predict_fn`, passed in via `prediction`, back to the SageMaker client in response to prediction requests\n",
    "\n",
    "Check the script below, which uses `chainer` to train on any number of GPUs on a single machine, to see how this works. This script implements `train` for training, and `model_fn`, `predict_fn`, `input_fn` and `output_fn` for hosting.\n",
    "\n",
    "For more on implementing these functions, see the documentation at https://github.com/aws/sagemaker-python-sdk.\n",
    "\n",
    "For more on the functions provided by the Chainer container, see https://github.com/aws/sagemaker-chainer-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat 'src/sentiment_analysis.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training script on SageMaker\n",
    "\n",
    "To train with a Chainer script, we construct a ```Chainer``` estimator using the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk). We can pass in an `entry_point`, the name of a script that contains a couple of functions with certain signatures (`train` and `model_fn`). This script will be run on SageMaker in a container that invokes these functions to train and load Chainer models.\n",
    "\n",
    "The ```Chainer``` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on a `ml.p3.2xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='sentiment_analysis.py', source_dir=\"src\", role=role,\n",
    "                            sagemaker_session=sagemaker_session,\n",
    "                            train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n",
    "                            hyperparameters={'epochs': 10, 'batch_size': 64})\n",
    "\n",
    "chainer_estimator.fit({'train': train_input, 'test': test_input, 'vocab': vocab_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Chainer script writes various artifacts, such as plots, to a directory `output_data_dir`, the contents of which which SageMaker uploads to S3. Now we download and extract these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_util import retrieve_output_from_s3\n",
    "\n",
    "chainer_training_job = chainer_estimator.latest_training_job.name\n",
    "\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=chainer_training_job)\n",
    "output_data = desc['ModelArtifacts']['S3ModelArtifacts'].replace('model.tar.gz', 'output.tar.gz')\n",
    "\n",
    "retrieve_output_from_s3(output_data, 'output/sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the accuracy and loss over epochs.\n",
    "\n",
    "In our user script, `sentiment_analysis.py`, at the end of the `train` function. Our model overfits, but we save only the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "accuracy_graph = Image(filename = \"output/sentiment/algo-1/accuracy.png\", width=800, height=800)\n",
    "loss_graph = Image(filename = \"output/sentiment/algo-1/loss.png\", width=800, height=800)\n",
    "\n",
    "display(accuracy_graph, loss_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Trained Model\n",
    "\n",
    "After training, we use the Chainer estimator object to create and deploy a hosted prediction endpoint. We can use a CPU-based instance for inference (in this case an `ml.m4.xlarge`), even though we trained on GPU instances.\n",
    "\n",
    "The predictor object returned by `deploy` lets us call the new endpoint and perform inference on our sample images.\n",
    "\n",
    "At the end of training, `sentiment_analysis.py` saves the trained model, the vocabulary, and a dictionary of model properties that are used to reconstruct the model. These model artifacts are loaded in `model_fn` when the model is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using SageMaker Endpoint\n",
    "\n",
    "The Chainer predictor converts its input into a NumPy array, which it serializes and sends to the hosted model.\n",
    "The `predict_fn` in `sentiment_analysis.py` receives this NumPy array and uses the loaded model to make predictions on the input data, which it returns as a NumPy array back to the Chainer predictor.\n",
    "\n",
    "We predict against the hosted model on a batch of sentences. The output, as defined by `predict_fn`, consists of the processed input sentence, the prediction, and the score for that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['It is fun and easy to train Chainer models on Amazon SageMaker!',\n",
    "             'It used to be slow, difficult, and laborious to train and deploy a model to production.',\n",
    "             'But now it is super fast to deploy to production. And I love it when my model generalizes!',]\n",
    "predictions = predictor.predict(sentences)\n",
    "for prediction in predictions:\n",
    "    sentence, prediction, score = prediction\n",
    "    print('sentence: {}\\nprediction: {}\\nscore: {}\\n'.format(sentence, prediction, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict against sentences in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_paths[1], 'r') as f:\n",
    "    sentences = f.readlines(2000)\n",
    "    sentences = [sentence[1:].strip() for sentence in sentences]\n",
    "    predictions = predictor.predict(sentences)\n",
    "\n",
    "predictions = predictor.predict(sentences)\n",
    "\n",
    "for prediction in predictions:\n",
    "    sentence, prediction, score = prediction\n",
    "    print('sentence: {}\\nprediction: {}\\nscore: {}\\n'.format(sentence, prediction, score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.      amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
